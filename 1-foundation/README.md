# Part 1: Building Your First AI Chatbot

This is the code implementation for [The Beginner's Guide to Building with AI](https://terminalerror.substack.com/p/the-beginners-guide-to-building-with) blog post.

The code in this folder is already complete and ready to run. The tutorial walks through creating this code step-by-step, but since it's already here, you just need to set up your environment and run it!

## What You'll Learn

This tutorial teaches you how to build a simple AI chatbot using Python and Langchain. The key concept is the **Adapter Pattern** - a flexible design that lets you easily switch between different AI providers (Ollama, OpenAI, Anthropic) without changing your code.

## Prerequisites

You should have already followed the instructions in the [root README](../README.md) and have:

- Python 3.13+ installed
- `uv` package manager installed
- Cloned this repository
- Created and activated the virtual environment at the root level
- Navigated to the `1-foundation` directory

## Setup

### 1. Install Dependencies

This step is a little different for you since you cloned the git repo rather than created the files manually. You already did this in step 3 of the [root README](../README.md)

### 2. Set Up Environment Configuration

We need to create your `.env` file here in this directory so that you can store you configuration information. Since you cloned the git repo you can copy/duplicate the `Example.env` file and rename it to `.env` and then just add the configuration information.

For example:

```bash
# For local models using Ollama (default)
LLM_PROVIDER="ollama"
MODEL="llama3.2:3b"
```

**Alternative configurations:**

For OpenAI:

```bash
LLM_PROVIDER="openai"
MODEL="gpt-4"
API_KEY="your-openai-api-key-here"
```

For Anthropic:

```bash
LLM_PROVIDER="anthropic"
MODEL="claude-3-5-sonnet-latest"
API_KEY=your-anthropic-api-key-here
```

### 3. Additional Setup for Ollama (if using local models)

If you're using Ollama as your provider:

1. Install Ollama from [ollama.com/download](https://ollama.com/download)
2. Pull the model you want to use:

   ```bash
   ollama pull llama3.2:3b
   ```

## Project Structure

The tutorial creates this structure (which is already set up for you):

```
1-foundation/
      main.py              # Entry point - runs the chatbot
      modules/
         adapter.py       # Adapter class - handles provider switching
      .env                 # Your configuration (you create this)
```

## How It Works

### The Adapter Pattern

The [adapter.py](modules/adapter.py) file contains the `Adapter` class that:

1. Reads your `.env` configuration
2. Sets up a prompt template
3. Initializes the appropriate LLM provider based on your settings
4. Provides a simple `chat()` method to interact with any provider

### Main Application

The [main.py](main.py) file:

1. Loads environment variables
2. Creates an `Adapter` instance
3. Sends a query to the chatbot
4. Prints the response

## Running the Code

Once you've set up your `.env` file, make sure you're in the `1-foundation` directory and run:

```bash
python main.py
```

(or `python3 main.py` or `py main.py` it all depends on your OS)

You should see a computer joke generated by your chosen AI model!

## Experimenting

Try these modifications to learn more:

1. **Change the query** in [main.py:9](main.py#L9) to ask different questions
2. **Switch providers** by changing `LLM_PROVIDER` in your `.env` file
3. **Try different models** by changing the `MODEL` variable
4. **Modify the prompt template** in [adapter.py:13-15](adapter.py#L13-L15)

## Understanding the Code

### Key Concepts from the Tutorial

1. **Environment Variables**: Configuration without hardcoding values
2. **Prompt Templates**: Standardized way to format requests to the LLM
3. **Chain Pattern**: Langchain's prompt -> LLM -> parser pipeline
4. **Provider Abstraction**: One interface for multiple AI providers

### Code Highlights

- **Conditional LLM initialization** ([adapter.py:17-38](modules/adapter.py#L17-L38)): Chooses the right provider
- **Chain creation** ([adapter.py:43](modules/adapter.py#L43)): Links prompt, LLM, and output parser
- **Environment loading** ([main.py:4-5](main.py#L4-L5)): Reads your `.env` file

## Troubleshooting

**Error: "API key is required"**

- Make sure you've added `API_KEY=your-key-here` to your `.env` file for OpenAI/Anthropic

**Error: "Ollama connection failed"**

- Ensure Ollama is installed and running
- Check that you've pulled the model: `ollama pull llama3.2:3b`

**Error: "Module not found"**

- Make sure you've activated your virtual environment at the root level
- Navigate back to the root directory and run `uv add` command again to install dependencies
- Verify you're running `python main.py` from within the `1-foundation` directory

## Next Steps

Once you've got this working, you can:

- Read the [full tutorial](https://terminalerror.substack.com/p/the-beginners-guide-to-building-with) for detailed explanations
- Move on to Part 2 (coming soon) to convert this into an agent
- Experiment with different models and providers
- Modify the code to add new features

## Questions?

Check out the [blog post](https://terminalerror.substack.com/p/the-beginners-guide-to-building-with) for detailed explanations, or open an issue in this repository.
